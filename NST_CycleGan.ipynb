{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NST-CycleGan.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing libraries"
      ],
      "metadata": {
        "id": "1I7E7aUuhCoE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gI3-yEc9yKY7"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torch\n",
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import ones, zeros\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "vubxcbakg7yz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_abs_file_paths(dir_name):\n",
        "    for dir_path, _, filenames in os.walk(dir_name):\n",
        "        for f in filenames:\n",
        "            yield os.path.abspath(os.path.join(dir_path, f))\n",
        "\n",
        "\n",
        "class NSTDataset(Dataset):\n",
        "\n",
        "    def __init__(self, root_dir, train=True, transform=None):\n",
        "        self.transform = transform\n",
        "\n",
        "        mode = 'train' if train else 'test'\n",
        "        self.sampleA = [file for file in get_abs_file_paths(os.path.join(root_dir, f\"{mode}A\"))]\n",
        "        self.sampleB = [file for file in get_abs_file_paths(os.path.join(root_dir, f\"{mode}B\"))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return 1000\n",
        "        # return max(len(self.sampleA), len(self.sampleB))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sampleA_len = len(self.sampleA)\n",
        "        sampleB_len = len(self.sampleB)\n",
        "\n",
        "        imageA = self.transform(Image.open(self.sampleA[idx % sampleA_len]))\n",
        "        imageB = self.transform(Image.open(self.sampleB[idx % sampleB_len]))\n",
        "\n",
        "        return {'imageA': imageA, 'imageB': imageB}"
      ],
      "metadata": {
        "id": "nX4nGUXofZ1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generator and discriminator"
      ],
      "metadata": {
        "id": "VzOrj4-JhIto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResnetGenerator(nn.Module):\n",
        "\n",
        "    def __init__(self, n_input_channels=3, n_filters=64, n_output_channels=3, use_dropout=False):\n",
        "        super(ResnetGenerator, self).__init__()\n",
        "\n",
        "        # down_sampling\n",
        "\n",
        "        self.initial_conv = nn.Sequential(\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(n_input_channels, n_filters, kernel_size=7),\n",
        "            nn.InstanceNorm2d(n_filters),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.down_sampling1 = nn.Sequential(\n",
        "            nn.Conv2d(n_filters, n_filters * 2, kernel_size=3, padding=1, stride=2),\n",
        "            nn.InstanceNorm2d(n_filters * 2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.down_sampling2 = nn.Sequential(\n",
        "            nn.Conv2d(n_filters * 2, n_filters * 4, kernel_size=3, padding=1, stride=2),\n",
        "            nn.InstanceNorm2d(n_filters * 4),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # residual blocks\n",
        "\n",
        "        self.residual_blks = []\n",
        "        for _ in range(9):\n",
        "            self.residual_blks += [Residual(n_filters * 4, use_dropout)]\n",
        "        self.residual_blks = nn.Sequential(*self.residual_blks)\n",
        "\n",
        "        # up_sampling\n",
        "\n",
        "        self.up_sampling1 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(n_filters * 4, n_filters * 2, kernel_size=3, padding=1, stride=2, output_padding=1),\n",
        "            nn.InstanceNorm2d(n_filters * 2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.up_sampling2 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(n_filters * 2, n_filters, kernel_size=3, padding=1, stride=2, output_padding=1),\n",
        "            nn.InstanceNorm2d(n_filters),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.final_conv = nn.Sequential(\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(n_filters, n_output_channels, kernel_size=7),\n",
        "            nn.InstanceNorm2d(n_output_channels),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = self.initial_conv(X)\n",
        "        X = self.down_sampling1(X)\n",
        "        X = self.down_sampling2(X)\n",
        "\n",
        "        X = self.residual_blks(X)\n",
        "\n",
        "        X = self.up_sampling1(X)\n",
        "        X = self.up_sampling2(X)\n",
        "        X = self.final_conv(X)\n",
        "\n",
        "        return X\n",
        "\n",
        "\n",
        "class Residual(nn.Module):\n",
        "\n",
        "    def __init__(self, n_channels, use_dropout=False):\n",
        "        super(Residual, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(n_channels, n_channels,\n",
        "                               kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(n_channels, n_channels,\n",
        "                               kernel_size=3, padding=1)\n",
        "        if use_dropout:\n",
        "            self.dropout = nn.Dropout(0.5)\n",
        "        else:\n",
        "            self.dropout = None\n",
        "\n",
        "        self.bn = nn.InstanceNorm2d(n_channels)\n",
        "\n",
        "    def forward(self, X):\n",
        "        Y = F.relu(self.bn(self.conv1(X)))\n",
        "        if self.dropout:\n",
        "            Y = self.dropout(Y)\n",
        "        Y = self.bn(self.conv2(Y))\n",
        "        Y += X\n",
        "        return F.relu(Y)\n",
        "\n",
        "\n",
        "class PatchGanDiscriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, n_input_channels=3, n_filters=64):\n",
        "        super(PatchGanDiscriminator, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(n_input_channels, n_filters, kernel_size=4, stride=2, padding=1)\n",
        "        self.bn = nn.InstanceNorm2d(n_filters)\n",
        "        self.main = []\n",
        "        for _ in range(3):\n",
        "            prev_channels = n_filters\n",
        "            n_filters *= 2\n",
        "            self.main += [nn.Conv2d(prev_channels, n_filters, kernel_size=4, stride=2, padding=1),\n",
        "                          nn.InstanceNorm2d(n_filters),\n",
        "                          nn.LeakyReLU(0.2)]\n",
        "\n",
        "        self.main += [nn.Conv2d(n_filters, n_filters, kernel_size=4, stride=2, padding=1),\n",
        "                      nn.InstanceNorm2d(n_filters),\n",
        "                      nn.LeakyReLU(0.2)]\n",
        "        self.main = nn.Sequential(*self.main)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(n_filters, 1, kernel_size=4, padding=1)\n",
        "        self.adap_pooling = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = F.leaky_relu_(self.bn(self.conv1(X)))\n",
        "        X = self.main(X)\n",
        "        X = self.conv2(X)\n",
        "        X = self.adap_pooling(X)\n",
        "\n",
        "        return torch.flatten(X)"
      ],
      "metadata": {
        "id": "1KPbqFqVyTYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Losses"
      ],
      "metadata": {
        "id": "L3F2zNKqhQIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CycleGanLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, discriminator_A, discriminator_B, gen_A2B, gen_B2A, device):\n",
        "        super(CycleGanLoss, self).__init__()\n",
        "\n",
        "        self.discriminator_A = discriminator_A\n",
        "        self.discriminator_B = discriminator_B\n",
        "        self.gen_A2B = gen_A2B\n",
        "        self.gen_B2A = gen_B2A\n",
        "\n",
        "        self.mse_loss = nn.MSELoss().to(device)  # losses for generator and discriminator\n",
        "        self.mae_loss = nn.L1Loss().to(device)  # identity and cycle losses\n",
        "\n",
        "    def calc(self, real_A, real_B):\n",
        "        real_labels, fake_labels = ones(real_A.size(0)), zeros(real_A.size(0))\n",
        "\n",
        "        def discriminator_loss(disc_real_results, disc_gen_results):\n",
        "            real_loss = self.mse_loss(disc_real_results, real_labels)\n",
        "            generated_loss = self.mse_loss(disc_gen_results, fake_labels)\n",
        "\n",
        "            return (real_loss + generated_loss) * 0.5\n",
        "\n",
        "        generated_B = self.gen_A2B(real_A)\n",
        "        generated_A = self.gen_B2A(real_B)\n",
        "\n",
        "        cycled_A = self.gen_B2A(generated_B)\n",
        "        cycled_B = self.gen_A2B(generated_A)\n",
        "\n",
        "        identical_A = self.gen_B2A(real_A)\n",
        "        identical_B = self.gen_A2B(real_B)\n",
        "\n",
        "        disc_real_A_results = self.discriminator_A(real_A)\n",
        "        disc_real_B_results = self.discriminator_B(real_B)\n",
        "\n",
        "        disc_gen_A_results = self.discriminator_A(generated_A)\n",
        "        disc_gen_B_results = self.discriminator_B(generated_B)\n",
        "\n",
        "        gen_A2B_loss = self.mse_loss(disc_gen_A_results, real_labels)\n",
        "        gen_B2A_loss = self.mse_loss(disc_gen_B_results, real_labels)\n",
        "\n",
        "        cycle_A_loss = self.mae_loss(cycled_A, real_A) * 10\n",
        "        cycled_B_loss = self.mae_loss(cycled_B, real_B) * 10\n",
        "\n",
        "        total_cycle_loss = cycle_A_loss + cycled_B_loss\n",
        "\n",
        "        identity_A_loss = self.mae_loss(real_A, identical_A) * 0.5\n",
        "        identity_B_loss = self.mae_loss(real_B, identical_B) * 0.5\n",
        "\n",
        "        # Total generator loss = adversarial loss + cycle loss\n",
        "        total_gen_loss = gen_A2B_loss + gen_B2A_loss + total_cycle_loss \\\n",
        "                         + identity_A_loss + identity_B_loss\n",
        "\n",
        "        disc_x_loss = discriminator_loss(disc_real_A_results, disc_gen_A_results)\n",
        "        disc_y_loss = discriminator_loss(disc_real_B_results, disc_gen_B_results)\n",
        "\n",
        "        total_loss = total_gen_loss + disc_x_loss + disc_y_loss\n",
        "\n",
        "        return total_loss\n"
      ],
      "metadata": {
        "id": "aAHyLgxMgYx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "5pL0VcTZhWRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "dataset_name = \"monet2photo\"\n",
        "\n",
        "url = f\"https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/{dataset_name}.zip\"\n",
        "output = \"dataset.zip\"\n",
        "gdown.download(url, output, False)"
      ],
      "metadata": {
        "id": "eWxH6LUqmg6B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "d8d82906-849d-4560-d91d-f55ca0e3d844"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/monet2photo.zip\n",
            "To: /content/dataset.zip\n",
            "100%|██████████| 305M/305M [02:35<00:00, 1.96MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'dataset.zip'"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "!unzip \"dataset.zip\"\n",
        "!rm \"dataset.zip\""
      ],
      "metadata": {
        "id": "HwAuITDknpdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HODvk3fMjieg",
        "outputId": "b49c013c-9fb3-4985-d0fc-901870694dd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "torch.manual_seed(17)\n",
        "image_size = 256\n",
        "epoch_n = 40\n",
        "freq_n = 1\n",
        "start_epoch = 0\n",
        "\n",
        "model_path = \"\"\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "models_dir = f\"/content/drive/MyDrive/NST-CycleGan/models\"\n",
        "dataset_dir = dataset_name\n",
        "images_storage_dir = f\"/content/drive/MyDrive/NST-CycleGan/images_output\"\n",
        "\n",
        "dataset = NSTDataset(root_dir=dataset_dir, train=True, transform=transform)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "\n",
        "gen_A2B = ResnetGenerator().to(device)\n",
        "gen_B2A = ResnetGenerator().to(device)\n",
        "discriminator_A = PatchGanDiscriminator().to(device)\n",
        "discriminator_B = PatchGanDiscriminator().to(device)\n",
        "\n",
        "optimizer_gen_A2B = torch.optim.Adam(gen_A2B.parameters())\n",
        "optimizer_gen_B2A = torch.optim.Adam(gen_B2A.parameters())\n",
        "optimizer_disc_A = torch.optim.Adam(discriminator_A.parameters())\n",
        "optimizer_disc_B = torch.optim.Adam(discriminator_B.parameters())\n",
        "\n",
        "if model_path is not \"\":\n",
        "  checkpoint = torch.load(model_path)\n",
        "  gen_A2B.load_state_dict(checkpoint['gen_A2B'])\n",
        "  optimizer_gen_A2B.load_state_dict(checkpoint['optimizer_gen_A2B'])\n",
        "  gen_B2A.load_state_dict(checkpoint['gen_B2A'])\n",
        "  optimizer_gen_B2A.load_state_dict(checkpoint['optimizer_gen_B2A'])\n",
        "  discriminator_A.load_state_dict(checkpoint['discriminator_A'])\n",
        "  optimizer_disc_A.load_state_dict(checkpoint['optimizer_disc_A'])\n",
        "  discriminator_B.load_state_dict(checkpoint['discriminator_B'])\n",
        "  optimizer_disc_B.load_state_dict(checkpoint['optimizer_disc_B'])\n",
        "  start_epoch = checkpoint['epoch']\n",
        "\n",
        "\n",
        "cycleGanLoss = CycleGanLoss(discriminator_A=discriminator_A, discriminator_B=discriminator_B,\n",
        "                            gen_A2B=gen_A2B, gen_B2A=gen_B2A, device=device)\n",
        "\n",
        "\n",
        "for epoch in range(start_epoch, epoch_n):\n",
        "    p_bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
        "    for idx, batch in p_bar:\n",
        "\n",
        "        optimizer_gen_A2B.zero_grad()\n",
        "        optimizer_gen_B2A.zero_grad()\n",
        "        optimizer_disc_A.zero_grad()\n",
        "        optimizer_disc_B.zero_grad()\n",
        "\n",
        "        image_A = batch['imageA']\n",
        "        image_B = batch['imageB']\n",
        "\n",
        "        cur_loss = cycleGanLoss.calc(image_A, image_B)\n",
        "        cur_loss.backward()\n",
        "\n",
        "        optimizer_gen_A2B.step()\n",
        "        optimizer_gen_B2A.step()\n",
        "        optimizer_disc_A.step()\n",
        "        optimizer_disc_B.step()\n",
        "\n",
        "        p_bar.set_description(\n",
        "            f\"[{epoch}/{epoch_n - 1}][{idx}/{len(dataloader) - 1}] \"\n",
        "            f\"total_loss: {cur_loss.item():.4f} \")\n",
        "\n",
        "    if epoch % freq_n == 0:\n",
        "      torch.save({\n",
        "      'epoch': epoch,\n",
        "      'gen_A2B': gen_A2B.state_dict(),\n",
        "      'optimizer_gen_A2B': optimizer_gen_A2B.state_dict(),\n",
        "      'gen_B2A': gen_B2A.state_dict(),\n",
        "      'optimizer_gen_B2A': optimizer_gen_B2A.state_dict(),\n",
        "      'discriminator_A': discriminator_A.state_dict(),\n",
        "      'optimizer_disc_A': optimizer_disc_A.state_dict(),\n",
        "      'discriminator_B': discriminator_B.state_dict(),\n",
        "      'optimizer_disc_B': optimizer_disc_B.state_dict(),\n",
        "      }, f\"{models_dir}/epoch_{epoch}_model.pth\")\n",
        "\n",
        "      save_image(image_A, f\"{images_storage_dir}/epoch_{epoch}_real_A.png\")\n",
        "      save_image(image_B, f\"{images_storage_dir}/epoch_{epoch}_real_B.png\")\n",
        "\n",
        "      image_gen_A = (gen_A2B(image_A).data * 0.5) + 0.5\n",
        "      image_gen_B = (gen_B2A(image_B).data * 0.5) + 0.5\n",
        "\n",
        "      save_image(image_gen_A, f\"{images_storage_dir}/epoch_{epoch}_gen_A.png\")\n",
        "      save_image(image_gen_B, f\"{images_storage_dir}/epoch_{epoch}_gen_B.png\")\n",
        "\n"
      ],
      "metadata": {
        "id": "v5oJmGVOgfDh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}